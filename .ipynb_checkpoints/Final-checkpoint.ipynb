{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f8631-b77a-4d5e-b4d2-c3efc45deab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "EXPORT_DIR = \"exports\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "spark = SparkSession.builder.appName(\"StudentMentalHealthBigData\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b188b-d792-4bbb-927e-89688a2268e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"students_mental_health.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86c5e0-eb03-4fec-a047-b0a87166e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_counts.show()\n",
    "missing_counts_pd = missing_counts.toPandas().T\n",
    "missing_counts_pd.columns = ['Missing']\n",
    "missing_counts_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4c48f-5f37-4f53-94c1-b51e5f705498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show stats and value counts for Depression_Score\n",
    "df.select(\"Depression_Score\").summary().show()\n",
    "df.groupBy(\"Depression_Score\").count().orderBy(\"Depression_Score\").show()\n",
    "\n",
    "# Compute quantiles for labels\n",
    "quantiles = df.approxQuantile(\"Depression_Score\", [0.33, 0.5, 0.66], 0.01)\n",
    "q33, median, q66 = quantiles\n",
    "print(f\"Quantiles: 33%={q33}, median={median}, 66%={q66}\")\n",
    "\n",
    "# Binary label for classification (above/below median)\n",
    "df = df.withColumn(\"Depression_Class\", (F.col(\"Depression_Score\") > median).cast(\"int\"))\n",
    "\n",
    "# Multi-class label: Low, Moderate, High\n",
    "df = df.withColumn(\n",
    "    \"Depression_Level\",\n",
    "    F.when(F.col(\"Depression_Score\") <= q33, \"Low\")\n",
    "     .when((F.col(\"Depression_Score\") > q33) & (F.col(\"Depression_Score\") <= q66), \"Moderate\")\n",
    "     .otherwise(\"High\")\n",
    ")\n",
    "df.groupBy(\"Depression_Class\").count().show()\n",
    "df.groupBy(\"Depression_Level\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb39897-1cd9-4d2b-83f8-c785632042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_columns = ['Age', 'CGPA', 'Depression_Score', 'Anxiety_Score']\n",
    "df = df.na.drop(subset=key_columns)\n",
    "fillna_cols = [\n",
    "    'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality',\n",
    "    'Social_Support', 'Relationship_Status', 'Substance_Use',\n",
    "    'Counseling_Service_Use', 'Family_History', 'Chronic_Illness',\n",
    "    'Extracurricular_Involvement', 'Residence_Type'\n",
    "]\n",
    "for c in fillna_cols:\n",
    "    df = df.fillna({c: \"Unknown\"})\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "categorical_cols = fillna_cols + ['Course']\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid='keep') for c in categorical_cols]\n",
    "for indexer in indexers:\n",
    "    df = indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[c+\"_idx\" for c in categorical_cols],\n",
    "    outputCols=[c+\"_onehot\" for c in categorical_cols]\n",
    ")\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "feature_cols = [\n",
    "    'Age', 'CGPA', 'Stress_Level', 'Depression_Score', 'Anxiety_Score',\n",
    "    'Financial_Stress', 'Semester_Credit_Load'\n",
    "] + [c+\"_idx\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "df = assembler.transform(df)\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(df)\n",
    "df = scalerModel.transform(df)\n",
    "\n",
    "onehot_features = [\n",
    "    'Age', 'CGPA', 'Stress_Level', 'Depression_Score', 'Anxiety_Score',\n",
    "    'Financial_Stress', 'Semester_Credit_Load'\n",
    "] + [c+\"_onehot\" for c in categorical_cols]\n",
    "assembler2 = VectorAssembler(inputCols=onehot_features, outputCol=\"features_onehot\")\n",
    "df = assembler2.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e155304-3cff-4881-9b3a-e4064a64b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "categorical_cols = fillna_cols + ['Course']\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid='keep') for c in categorical_cols]\n",
    "for indexer in indexers:\n",
    "    df = indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(inputCols=[c+\"_idx\" for c in categorical_cols], outputCols=[c+\"_onehot\" for c in categorical_cols])\n",
    "df = encoder.fit(df).transform(df)\n",
    "df.select(['Gender', 'Gender_idx', 'Gender_onehot']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cdde79-5755-44d3-8ee6-4cd3c6a1c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "feature_cols = [\n",
    "    'Age', 'CGPA', 'Stress_Level', 'Depression_Score', 'Anxiety_Score',\n",
    "    'Financial_Stress', 'Semester_Credit_Load'\n",
    "] + [c+\"_idx\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "df = assembler.transform(df)\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(df)\n",
    "df = scalerModel.transform(df)\n",
    "\n",
    "onehot_features = ['Age', 'CGPA', 'Stress_Level', 'Depression_Score', 'Anxiety_Score', 'Financial_Stress', 'Semester_Credit_Load'] + [c+\"_onehot\" for c in categorical_cols]\n",
    "assembler2 = VectorAssembler(inputCols=onehot_features, outputCol=\"features_onehot\")\n",
    "df = assembler2.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5396ec-8341-40b2-9970-33e65c03cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary = df.select(['Age','CGPA','Stress_Level','Depression_Score','Anxiety_Score','Financial_Stress']).summary()\n",
    "summary.show()\n",
    "summary_pd = summary.toPandas()\n",
    "display(summary_pd)\n",
    "\n",
    "# Barplot Gender\n",
    "gender_counts = df.groupBy('Gender').count().toPandas()\n",
    "sns.barplot(data=gender_counts, x='Gender', y='count')\n",
    "plt.title('Gender Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Sleep Quality\n",
    "sleep_counts = df.groupBy('Sleep_Quality').count().toPandas()\n",
    "sns.barplot(data=sleep_counts, x='Sleep_Quality', y='count')\n",
    "plt.title('Sleep Quality Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Histograms, Boxplots\n",
    "numeric_cols = ['Age','CGPA','Stress_Level','Depression_Score','Anxiety_Score','Financial_Stress']\n",
    "pd_df = df.select(numeric_cols).toPandas()\n",
    "for col in numeric_cols:\n",
    "    sns.histplot(pd_df[col].dropna(), kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()\n",
    "    sns.boxplot(x=pd_df[col])\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "    plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "corr = pd_df.corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d28fb6-25ef-4431-ad77-e61a41810bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"students\")\n",
    "\n",
    "# 1. Simple SELECT & WHERE\n",
    "result1 = spark.sql(\"SELECT Age, Gender, Depression_Score FROM students WHERE Depression_Score > 7\")\n",
    "result1.show(5)\n",
    "\n",
    "# 2. GROUP BY aggregation\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT Gender, COUNT(*) as count, AVG(Depression_Score) as avg_dep\n",
    "    FROM students\n",
    "    GROUP BY Gender\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "result2.show()\n",
    "\n",
    "# 3. DISTINCT count\n",
    "result3 = spark.sql(\"SELECT COUNT(DISTINCT Course) as unique_courses FROM students\")\n",
    "result3.show()\n",
    "\n",
    "# 4. Cross-tab\n",
    "result4 = spark.sql(\"\"\"\n",
    "    SELECT Sleep_Quality, COUNT(*) as count\n",
    "    FROM students\n",
    "    GROUP BY Sleep_Quality\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "result4.show()\n",
    "\n",
    "# 5. Multi-condition filter\n",
    "result5 = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM students\n",
    "    WHERE Depression_Score > 7 AND Anxiety_Score > 7 AND Sleep_Quality = 'Poor'\n",
    "    ORDER BY Age DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "result5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f00cf-b249-4ed3-8da1-2844537849ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) Average and Standard Deviation of CGPA by Course\n",
    "result6 = spark.sql(\"\"\"\n",
    "    SELECT Course, COUNT(*) as n, AVG(CGPA) as avg_cgpa, STDDEV(CGPA) as std_cgpa\n",
    "    FROM students\n",
    "    GROUP BY Course\n",
    "    ORDER BY avg_cgpa DESC\n",
    "\"\"\")\n",
    "result6.show()\n",
    "\n",
    "# (B) Gender-wise Depression and Anxiety Score Distributions\n",
    "result7 = spark.sql(\"\"\"\n",
    "    SELECT Gender, AVG(Depression_Score) as avg_depression, AVG(Anxiety_Score) as avg_anxiety\n",
    "    FROM students\n",
    "    GROUP BY Gender\n",
    "    ORDER BY avg_depression DESC\n",
    "\"\"\")\n",
    "result7.show()\n",
    "\n",
    "# (C) Crosstab of Residence Type and Sleep Quality\n",
    "result8 = spark.sql(\"\"\"\n",
    "    SELECT Residence_Type, Sleep_Quality, COUNT(*) as count\n",
    "    FROM students\n",
    "    GROUP BY Residence_Type, Sleep_Quality\n",
    "    ORDER BY Residence_Type, count DESC\n",
    "\"\"\")\n",
    "result8.show()\n",
    "\n",
    "# (D) Proportion of High Depression Risk by Social Support\n",
    "result9 = spark.sql(\"\"\"\n",
    "    SELECT Social_Support,\n",
    "           COUNT(*) as total,\n",
    "           SUM(CASE WHEN Depression_Score >= 7 THEN 1 ELSE 0 END) as high_risk,\n",
    "           ROUND(100.0 * SUM(CASE WHEN Depression_Score >= 7 THEN 1 ELSE 0 END)/COUNT(*),2) as high_risk_pct\n",
    "    FROM students\n",
    "    GROUP BY Social_Support\n",
    "    ORDER BY high_risk_pct DESC\n",
    "\"\"\")\n",
    "result9.show()\n",
    "\n",
    "# (E) Top 10 Students by Stress Level and CGPA\n",
    "result10 = spark.sql(\"\"\"\n",
    "    SELECT Age, Gender, Course, CGPA, Stress_Level\n",
    "    FROM students\n",
    "    ORDER BY Stress_Level DESC, CGPA DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "result10.show()\n",
    "\n",
    "# (F) Distribution of Physical Activity by High Depression Risk\n",
    "result11 = spark.sql(\"\"\"\n",
    "    SELECT Physical_Activity,\n",
    "           SUM(CASE WHEN Depression_Score >= 7 THEN 1 ELSE 0 END) as high_risk,\n",
    "           COUNT(*) as total,\n",
    "           ROUND(100.0 * SUM(CASE WHEN Depression_Score >= 7 THEN 1 ELSE 0 END)/COUNT(*),2) as pct_high_risk\n",
    "    FROM students\n",
    "    GROUP BY Physical_Activity\n",
    "    ORDER BY pct_high_risk DESC\n",
    "\"\"\")\n",
    "result11.show()\n",
    "\n",
    "# (G) Gender vs Course vs High Depression Risk\n",
    "result12 = spark.sql(\"\"\"\n",
    "    SELECT Gender, Course,\n",
    "           SUM(CASE WHEN Depression_Score >= 7 THEN 1 ELSE 0 END) as high_risk,\n",
    "           COUNT(*) as total,\n",
    "           ROUND(100.0 * SUM(CASE WHEN Depression_Score >= 7 THEN 1 ELSE 0 END)/COUNT(*),2) as pct_high_risk\n",
    "    FROM students\n",
    "    GROUP BY Gender, Course\n",
    "    ORDER BY pct_high_risk DESC\n",
    "\"\"\")\n",
    "result12.show()\n",
    "\n",
    "# (H) Fairness Table: High Depression Risk by Gender\n",
    "result13 = spark.sql(\"\"\"\n",
    "    SELECT Gender,\n",
    "           SUM(CASE WHEN Depression_Score >= 7 THEN 1 ELSE 0 END) as high_risk,\n",
    "           COUNT(*) as total,\n",
    "           ROUND(100.0 * SUM(CASE WHEN Depression_Score >= 7 THEN 1 ELSE 0 END)/COUNT(*),2) as pct_high_risk\n",
    "    FROM students\n",
    "    GROUP BY Gender\n",
    "    ORDER BY pct_high_risk DESC\n",
    "\"\"\")\n",
    "result13.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26504a5f-9ecf-439d-a145-b927ba6e53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "kmeans = KMeans(featuresCol='features', k=3, seed=42)\n",
    "kmeans_model = kmeans.fit(df)\n",
    "df = kmeans_model.transform(df)\n",
    "\n",
    "clustering_evaluator = ClusteringEvaluator(featuresCol='features', predictionCol='prediction', metricName='silhouette')\n",
    "silhouette = clustering_evaluator.evaluate(df)\n",
    "print(\"KMeans Silhouette Score:\", silhouette)\n",
    "\n",
    "# PCA for 2D visualization (optional for plotting)\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df)\n",
    "df = pca_model.transform(df)\n",
    "pca_pd = df.select('pca_features','prediction').toPandas()\n",
    "pca_pd['PCA1'] = pca_pd['pca_features'].apply(lambda x: x[0])\n",
    "pca_pd['PCA2'] = pca_pd['pca_features'].apply(lambda x: x[1])\n",
    "sns.scatterplot(data=pca_pd, x='PCA1', y='PCA2', hue='prediction', palette='tab10')\n",
    "plt.title(\"KMeans Clusters (PCA 2D)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f2a08-0cc1-4dc5-b915-cddb69651257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster means for each numeric col\n",
    "for col in numeric_cols:\n",
    "    means = df.groupBy('prediction').agg(F.avg(col).alias('mean')).orderBy('prediction').toPandas()\n",
    "    plt.bar(means['prediction'], means['mean'])\n",
    "    plt.title(f\"Cluster Means: {col}\")\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(f\"Mean {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38480c-ac60-4565-9e3c-3697fe87a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure both classes exist for Depression_Class\n",
    "df.groupBy(\"Depression_Class\").count().show()\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "train_class_counts = train_df.groupBy(\"Depression_Class\").count().toPandas()\n",
    "if len(train_class_counts) < 2:\n",
    "    print(\"Warning: Training set contains only one class. Classification skipped.\")\n",
    "else:\n",
    "    for colname in ['prediction']:\n",
    "        if colname in train_df.columns:\n",
    "            train_df = train_df.drop(colname)\n",
    "        if colname in test_df.columns:\n",
    "            test_df = test_df.drop(colname)\n",
    "\n",
    "    from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "\n",
    "    log_reg = LogisticRegression(featuresCol=\"features\", labelCol=\"Depression_Class\", maxIter=20)\n",
    "    log_model = log_reg.fit(train_df)\n",
    "    preds_lr = log_model.transform(test_df)\n",
    "\n",
    "    rf = RandomForestClassifier(featuresCol=\"features_onehot\", labelCol=\"Depression_Class\", numTrees=50)\n",
    "    rf_model = rf.fit(train_df)\n",
    "    preds_rf = rf_model.transform(test_df)\n",
    "\n",
    "    gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"Depression_Class\", maxIter=30)\n",
    "    gbt_model = gbt.fit(train_df)\n",
    "    preds_gbt = gbt_model.transform(test_df)\n",
    "\n",
    "    print(\"Classification completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ba188-874a-4552-bdb4-e39d32a5017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "def print_classification_metrics(preds, model_name=\"\", skip_roc_auc=False):\n",
    "    if not skip_roc_auc:\n",
    "        evaluator_roc = BinaryClassificationEvaluator(\n",
    "            rawPredictionCol=\"rawPrediction\", labelCol=\"Depression_Class\", metricName=\"areaUnderROC\"\n",
    "        )\n",
    "        try:\n",
    "            roc_auc = evaluator_roc.evaluate(preds)\n",
    "        except Exception:\n",
    "            roc_auc = None\n",
    "    else:\n",
    "        roc_auc = None\n",
    "\n",
    "    evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"Depression_Class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator_acc.evaluate(preds)\n",
    "    precision = MulticlassClassificationEvaluator(labelCol=\"Depression_Class\", predictionCol=\"prediction\", metricName=\"precisionByLabel\").evaluate(preds)\n",
    "    recall = MulticlassClassificationEvaluator(labelCol=\"Depression_Class\", predictionCol=\"prediction\", metricName=\"recallByLabel\").evaluate(preds)\n",
    "    f1 = MulticlassClassificationEvaluator(labelCol=\"Depression_Class\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(preds)\n",
    "    if roc_auc is not None:\n",
    "        print(f\"{model_name} -- ROC AUC: {roc_auc:.3f}, Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "    else:\n",
    "        print(f\"{model_name} -- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f} (ROC AUC not available)\")\n",
    "    return [model_name, accuracy, precision, recall, f1, roc_auc]\n",
    "\n",
    "results = []\n",
    "results.append(print_classification_metrics(preds_lr, \"Logistic Regression\"))\n",
    "results.append(print_classification_metrics(preds_rf, \"Random Forest\"))\n",
    "results.append(print_classification_metrics(preds_gbt, \"Gradient Boosted Trees\", skip_roc_auc=True))\n",
    "\n",
    "comp_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'ROC_AUC'])\n",
    "display(comp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca60fe1-0998-4cb5-8568-2fb2c1c09242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_spark_roc(predictions, label=''):\n",
    "    pred_pd = predictions.select('probability','High_Depression_Risk').toPandas()\n",
    "    pred_pd['prob1'] = pred_pd['probability'].apply(lambda x: x[1] if hasattr(x, \"__getitem__\") else float(x))\n",
    "    fpr, tpr, _ = roc_curve(pred_pd['High_Depression_Risk'], pred_pd['prob1'])\n",
    "    auc_val = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{label} (AUC={auc_val:.2f})')\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plot_spark_roc(preds_lr, 'Logistic Regression')\n",
    "plot_spark_roc(preds_rf, 'Random Forest')\n",
    "plot_spark_roc(preds_gbt, 'GBT')\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (All Models)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603911ea-3a89-469d-9b55-ade188f7b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "def print_regression_metrics(preds, model_name=\"\"):\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=\"Depression_Score_Reg\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=\"Depression_Score_Reg\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    rmse = evaluator_rmse.evaluate(preds)\n",
    "    r2 = evaluator_r2.evaluate(preds)\n",
    "    print(f\"{model_name} -- RMSE: {rmse:.3f}, RÂ²: {r2:.3f}\")\n",
    "    return [model_name, rmse, r2]\n",
    "\n",
    "reg_results = []\n",
    "reg_results.append(print_regression_metrics(preds_lr_reg, \"Linear Regression\"))\n",
    "reg_results.append(print_regression_metrics(preds_rf_reg, \"Random Forest Regressor\"))\n",
    "reg_results.append(print_regression_metrics(preds_gbt_reg, \"Gradient Boosted Trees Regressor\"))\n",
    "\n",
    "reg_comp_df = pd.DataFrame(reg_results, columns=['Model', 'RMSE', 'R2'])\n",
    "display(reg_comp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df13173-9a0e-42e4-b867-2f3bbbdb11aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.featureImportances.toArray()\n",
    "oh_cols = []\n",
    "for c in onehot_features:\n",
    "    if 'onehot' in c:\n",
    "        try:\n",
    "            n = df.select(c).head()[c].size\n",
    "        except:\n",
    "            n = 0\n",
    "        oh_cols.extend([c+f\"_{i}\" for i in range(n)])\n",
    "    else:\n",
    "        oh_cols.append(c)\n",
    "fi_df = pd.DataFrame({'Feature': oh_cols, 'Importance': importances})\n",
    "fi_df = fi_df.sort_values('Importance', ascending=False).head(15)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=fi_df, x='Importance', y='Feature')\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414a3d8-dbf1-4d6e-9cb7-7b140cdd42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cols = [\n",
    "    'Age', 'Gender', 'CGPA', 'Stress_Level', 'Depression_Score', 'Anxiety_Score',\n",
    "    'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support',\n",
    "    'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History',\n",
    "    'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type', 'Course',\n",
    "    'prediction', 'Depression_Class', 'Depression_Level'\n",
    "]\n",
    "final_export = df.select([c for c in out_cols if c in df.columns]).toPandas()\n",
    "final_export.to_excel(\"student_mental_health_all_for_tableau.xlsx\", index=False)\n",
    "print(\"Exported: student_mental_health_all_for_tableau.xlsx\")\n",
    "final_export.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
